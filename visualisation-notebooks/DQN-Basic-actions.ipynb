{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbed7540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] Die Auslagerungsdatei ist zu klein, um diesen Vorgang durchzuführen. Error loading \"C:\\Users\\satte\\anaconda3\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de16a9bbe641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# UNI BIG-DATA PROJECT / DQN IMPLEMENTIERUNG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] Die Auslagerungsdatei ist zu klein, um diesen Vorgang durchzuführen. Error loading \"C:\\Users\\satte\\anaconda3\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# UNI BIG-DATA PROJECT / DQN IMPLEMENTIERUNG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.02 # KLEINER MACHEN!!! beim nächsten Test, 0.01 zb\n",
    "num_episodes = 1000\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "####################\n",
    "goal_list = []\n",
    "goal = 0\n",
    "bars = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "action_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfad070",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh() # =nn.Sigmoid() AUCH TESTEN!!!!!\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "\n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #TODO: try other loss functions\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        #TODO try other optimizers\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0] \n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "reward_list = []\n",
    "ave_reward_list = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "goal = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    reward_list = []\n",
    "    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        tot_reward += reward\n",
    "        reward_list.append(tot_reward)\n",
    "        #env.render()\n",
    "        \n",
    "                \n",
    "        #Visualisierung Actionfarbpalette\n",
    "        x_val.append(new_state[0])\n",
    "        y_val.append(new_state[1])\n",
    "        action_list.append(action)\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            \n",
    "            if new_state[0] >= env.goal_position:\n",
    "                goal +=1\n",
    "\n",
    "            if (i_episode+1) % 100 == 0:\n",
    "                #ave_reward = np.mean(reward_list)\n",
    "                #ave_reward_list.append(ave_reward)\n",
    "                reward_list = []\n",
    "                goal_list.append(goal)\n",
    "                goal=0\n",
    "                bars.append(str(i_episode+1))\n",
    "                \n",
    "                #Visualisierung\n",
    "                action_list = pd.Series(action_list)\n",
    "        \n",
    "                colors = {0:'blue',1:'lime',2:'red'}\n",
    "                colors = action_list.apply(lambda x:colors[x])\n",
    "                labels = ['Left','Right','Nothing']\n",
    "\n",
    "                fig = plt.figure(figsize=[7,7])\n",
    "                ax = fig.gca()\n",
    "                plt.set_cmap('brg')\n",
    "                surf = ax.scatter(x_val,y_val, s= 10, c=action_list)\n",
    "                ax.set_xlabel('Position')\n",
    "                ax.set_ylabel('Velocity')\n",
    "                ax.set_title('Entwicklung der actions \\n der Episoden {} - {}'.format(i_episode+1-100, i_episode+1))\n",
    "                recs = []\n",
    "                for i in range(0,3):\n",
    "                     recs.append(mpatches.Rectangle((0,0),1,1,fc=sorted(colors.unique())[i]))\n",
    "                plt.legend(recs,labels,loc=4,ncol=3)\n",
    "                e = i_episode+1\n",
    "\n",
    "                # WICHTIG: vorher den Ordner \"actions\" anlegen!!!\n",
    "                plt.savefig(f\"actions_dqn/00{e}_dqn_actions.png\", facecolor='w', edgecolor='w') # f\"qtable_bilder/{i}.png\"\n",
    "                plt.show()\n",
    "                x_val = []\n",
    "                y_val = []\n",
    "                action_list = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            break\n",
    "\n",
    "            env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((np.arange(len(ave_reward_list)) + 1), ave_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward & Episodes')\n",
    "plt.savefig(f\"actions_dqn/ave_reward_dqn_actions.png\", facecolor='w', edgecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e12d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bars,goal_list, width = 0.8)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"goals\")\n",
    "plt.title(\"Goals per 100 Episodes\")\n",
    "plt.savefig(f\"actions_dqn/goals_per_100_ep_dqn_actions.png\", facecolor='w', edgecolor='w')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48857d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc479ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
